{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random \n",
    "import sys\n",
    "import io\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = {};\n",
    "#devtext = {}; development\n",
    "testtext = {};\n",
    "nameList = [\"Drake\",\"Queen\",\"Iron Maiden\",\"Eminem\"];\n",
    "\n",
    "# Opening JSON file \n",
    "f = open('data/lyricdataii.json',) \n",
    "\n",
    "# returns JSON object as  \n",
    "# a dictionary \n",
    "\n",
    "data = json.load(f)\n",
    "'''for i in range(int(len(data[name])*6/10)):\n",
    "    text += data[name][i];\n",
    "    \n",
    "for i in range(int(len(data[name])*6/10)+1,len(data[name])-1):\n",
    "    devtext += data[name][i];'''\n",
    "\n",
    "for name in nameList:\n",
    "    text[name] = \"\";\n",
    "    for i in range(len(data[name])-1):\n",
    "        text[name] += data[name][i];\n",
    "\n",
    "    testtext[name] = data[name][len(data[name])-1]\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Gram Model Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans the given string and returns it as a list of words, lower-cased\n",
    "def sent_transform(sent_string):\n",
    "    return nltk.word_tokenize(sent_string.lower())\n",
    "\n",
    "# words is a list of words\n",
    "def make_ngram_tuples(words, n):\n",
    "    for i in range(0,n):\n",
    "        words.insert(0, '<s>')\n",
    "    words.append('</s>')\n",
    "    result = []\n",
    "    x = range(n, len(words))\n",
    "    for i in x:\n",
    "        context = tuple(words[i-n+1:i])\n",
    "        sequence = (context,) + (words[i],)\n",
    "        result.append(sequence)\n",
    "    return result\n",
    "\n",
    "class RandomModel(object):\n",
    "    def __init__(self, song_list, name):\n",
    "        self.name = name\n",
    "        self.d = Counter() # keep track of # of words\n",
    "        for song in song_list:\n",
    "            sentences = nltk.sent_tokenize(song)\n",
    "            for sentence in sentences:\n",
    "                list_words = sent_transform(sentence)\n",
    "                for word in list_words:\n",
    "                    self.d[word] += 1\n",
    "        \n",
    "\n",
    "class BigramModel(object):\n",
    "    \n",
    "    def __init__(self, inputfile, name):\n",
    "        self.name = name\n",
    "        self.d = Counter() # keep track of # of words\n",
    "        \n",
    "        # iterating through the text file and incrementing the count for the words seen\n",
    "        f = open('data/' + inputfile, 'r')\n",
    "        text = f.read()\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        for line in sentences:\n",
    "            if(line != '\\n'):\n",
    "                list_words = sent_transform(line)\n",
    "                ngram_sequence = make_ngram_tuples(list_words, 2)\n",
    "                for e in ngram_sequence:\n",
    "                    self.d[\"\".join(e[0])] += 1\n",
    "                    self.d[e[1]] += 1\n",
    "        \n",
    "        self.unk_set = set() # set containing words that appear only once\n",
    "        self.dsum = 0 # the size of the vocabulary\n",
    "        \n",
    "        for word in self.d:\n",
    "            if self.d[word] == 1:\n",
    "                self.unk_set.add(word)\n",
    "            else:\n",
    "                self.dsum += 1\n",
    "        \n",
    "        self.d_bigram = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        f = open('data/' + inputfile, 'r')\n",
    "        text = f.read()\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        for line in sentences:\n",
    "            list_words = sent_transform(line) \n",
    "            ngram_sequence = make_ngram_tuples(list_words, 2)\n",
    "            for e in ngram_sequence:\n",
    "                context = \"\".join(e[0]) if \"\".join(e[0]) not in self.unk_set else \"<UNK>\"\n",
    "                word = e[1] if e[1] not in self.unk_set else \"<UNK>\"\n",
    "                self.d_bigram[context][word] += 1\n",
    "    \n",
    "    def logprob(self, context, word): \n",
    "        if context in self.unk_set:\n",
    "            context = \"<UNK>\"\n",
    "        if word in self.unk_set:\n",
    "            word = \"<UNK>\"\n",
    "        return np.log2(self.d_bigram[context][word]+1)-np.log2(self.d[context]+self.dsum)\n",
    "    \n",
    "    def get_ppl(self, testfile):\n",
    "        log_corpus_prob = 0\n",
    "        len_corpus = 0\n",
    "        with open('data/' + testfile) as f:\n",
    "            for line in f:\n",
    "                if(line != '\\n'):\n",
    "                    list_words = sent_transform(line)\n",
    "                    ngram_sequence = make_ngram_tuples(list_words, 2)\n",
    "                    for e in ngram_sequence:\n",
    "                        context = \"\".join(e[0]) if \"\".join(e[0]) not in self.unk_set else \"<UNK>\"\n",
    "                        word = e[1] if e[1] not in self.unk_set else \"<UNK>\"\n",
    "                        log_corpus_prob += self.logprob(context, word)\n",
    "                    len_corpus += len(list_words)\n",
    "        return 2 ** (-1 * log_corpus_prob/len_corpus)\n",
    "\n",
    "    \n",
    "    \n",
    "# generates n sentences given a random model\n",
    "def random_text_generator(randomlm, n):\n",
    "    f = open(\"data/\" + randomlm.name + \"-random-generated-song.txt\", \"w\")\n",
    "    word_count = randomlm.d\n",
    "    for i in range(0, n):\n",
    "        cur_sentence_len = 0\n",
    "        sentence = \"\"\n",
    "        # generates sentences of length 10\n",
    "        sorted_words = sorted(word_count, key=word_count.get)\n",
    "        len_sentence = random.randint(8, 15)\n",
    "        while cur_sentence_len <= 9:\n",
    "            sentence += random.choice(sorted_words[-100:]) + \" \"\n",
    "            cur_sentence_len += 1\n",
    "        f.write(sentence)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# generates n sentences based off of the bigram model\n",
    "def text_generator(bigramlm, n):\n",
    "    f = open(\"data/\" + bigramlm.name + \"-bigram-generated-song.txt\", \"w\")\n",
    "    bigram = bigramlm.d_bigram\n",
    "    for i in range(0, n):\n",
    "        len_sentence = 0\n",
    "        cur_word = \"<s>\"\n",
    "        sentence = \"\"\n",
    "        while len_sentence <= 13:\n",
    "            sorted_d = sorted((value, key) for (key, value) in bigram[cur_word].items())\n",
    "            #print(sorted_d)\n",
    "            if(len(sorted_d) >= 5):\n",
    "                next_word = sorted_d[len(sorted_d)-randint(1, 5)][1]\n",
    "            else:\n",
    "                next_word = sorted_d[randint(0, len(sorted_d)-1)][1]\n",
    "            if next_word == \"</s>\":\n",
    "                break\n",
    "            cur_word = next_word\n",
    "            sentence += next_word + \" \"\n",
    "            len_sentence += 1\n",
    "        f.write(sentence)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        \n",
    "drake_model = RandomModel(data['Drake'], 'drake')\n",
    "random_text_generator(drake_model, 10)\n",
    "        \n",
    "drake_bg_model = BigramModel('drake-songs.txt', 'drake')\n",
    "text_generator(drake_bg_model, 10)\n",
    "\n",
    "queen_model = RandomModel(data['Queen'], 'queen')\n",
    "random_text_generator(queen_model, 10)\n",
    "\n",
    "queen_bg_model = BigramModel('queen-songs.txt', 'queen')\n",
    "text_generator(queen_bg_model, 10)\n",
    "\n",
    "ironmaiden_model = RandomModel(data['Iron Maiden'], 'ironmaiden')\n",
    "random_text_generator(ironmaiden_model, 10)\n",
    "\n",
    "ironmaiden_bg_model = BigramModel('ironmaiden-songs.txt', 'ironmaiden')\n",
    "text_generator(ironmaiden_bg_model, 10)\n",
    "\n",
    "eminem_model = RandomModel(data['Eminem'], 'eminem')\n",
    "random_text_generator(eminem_model, 10)\n",
    "\n",
    "eminem_bg_model = BigramModel('eminem-songs.txt', 'eminem')\n",
    "text_generator(eminem_bg_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136.35836530796567\n",
      "1050.889190555876\n",
      "-----------------------------------------------\n",
      "94.95996887296059\n",
      "825.1638531454465\n",
      "-----------------------------------------------\n",
      "180.71346499838944\n",
      "500.5128051261902\n",
      "-----------------------------------------------\n",
      "151.67755301311522\n",
      "1591.6845699444023\n"
     ]
    }
   ],
   "source": [
    "print(drake_bg_model.get_ppl('drake-bigram-generated-song.txt'))\n",
    "print(drake_bg_model.get_ppl('drake-random-generated-song.txt'))\n",
    "print('-----------------------------------------------')\n",
    "print(queen_bg_model.get_ppl('queen-bigram-generated-song.txt'))\n",
    "print(queen_bg_model.get_ppl('queen-random-generated-song.txt'))\n",
    "print('-----------------------------------------------')\n",
    "print(ironmaiden_bg_model.get_ppl('ironmaiden-bigram-generated-song.txt'))\n",
    "print(ironmaiden_bg_model.get_ppl('ironmaiden-random-generated-song.txt'))\n",
    "print('-----------------------------------------------')\n",
    "print(eminem_bg_model.get_ppl('eminem-bigram-generated-song.txt'))\n",
    "print(eminem_bg_model.get_ppl('eminem-random-generated-song.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = {}\n",
    "\n",
    "for name in nameList:\n",
    "    processed_text[name] = text[name].lower() # turns all the text into lowercase\n",
    "    processed_text[name] = re.sub(r'[^\\x00-\\x7f]',r'', processed_text[name]) #only include ascii characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = {}\n",
    "char_indices = {}\n",
    "indices_char = {}\n",
    "\n",
    "for name in nameList:\n",
    "    chars[name] = sorted(list(set(processed_text[name]))) #gives a sorted list of all the characters that appear in your text\n",
    "    char_indices[name] = dict((c,i) for i, c in enumerate(chars[name])) # dictionary that's in this form: \"character\": index of the character\n",
    "    indices_char[name] = dict((i,c) for i, c in enumerate(chars[name])) # dictionary that's in this form: index: \"character\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 102700\n",
      "nb sequences: 113460\n",
      "nb sequences: 22222\n",
      "nb sequences: 214800\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = {}\n",
    "next_chars = {}\n",
    "\n",
    "\n",
    "for name in nameList:\n",
    "    sentences[name] = []\n",
    "    next_chars[name] = []\n",
    "    \n",
    "    # the program goes through all the text from left to right, records a string of (maxlen) in sentences, \n",
    "    # and the character that comes next in next_chars\n",
    "    for i in range(0, len(processed_text[name]) - maxlen, step):\n",
    "        sentences[name].append(processed_text[name][i: i + maxlen])\n",
    "        next_chars[name].append(processed_text[name][i + maxlen])\n",
    "    print(\"nb sequences:\", len(sentences[name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "y = {}\n",
    "\n",
    "for name in nameList:\n",
    "\n",
    "    # Now vectorize the data\n",
    "\n",
    "    x[name] = np.zeros((len(sentences[name]), maxlen, len(chars[name])), dtype=np.bool) # 3D array of len(sentences) x maxlen x len(chars)\n",
    "    y[name] = np.zeros((len(sentences[name]), len(chars[name])), dtype=np.bool) # 2D array of len(sentences) x len(chars)\n",
    "    # x and ys are all filled with false at this point\n",
    "\n",
    "    for i, sentence in enumerate(sentences[name]):\n",
    "        #i is the index\n",
    "        #sentence is whatever is in sentences[i]... a fragment of the lyrics \n",
    "        for t, char in enumerate(sentence):\n",
    "            x[name][i, t, char_indices[name][char]] = 1 \n",
    "            # your turning the string into a giant array. \n",
    "            # its like for \"a\" youd have [true, false, false,... ....false,false] and so on, for every char \n",
    "            # in every fragment in sentences \n",
    "        y[name][i, char_indices[name][next_chars[name][i]]] = 1\n",
    "        # and y is just x but for next_chars instead of sentences\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102700, 40, 59)\n",
      "(113460, 40, 53)\n",
      "(22222, 40, 50)\n",
      "(214800, 40, 62)\n"
     ]
    }
   ],
   "source": [
    "for name in nameList:\n",
    "    print(x[name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102700, 59)\n",
      "(113460, 53)\n",
      "(22222, 50)\n",
      "(214800, 62)\n"
     ]
    }
   ],
   "source": [
    "for name in nameList:\n",
    "    print(y[name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "\n",
    "for name in nameList:\n",
    "\n",
    "    model[name] = Sequential() # ok now create the model\n",
    "    model[name].add(LSTM(128, input_shape=(maxlen, len(chars[name])))) \n",
    "    #add a LSTM layer to the model... input shape is \n",
    "    #what kind of array is it expecting, in this case, a maxlen * len(chars) array... aka a vectorize lyric fragment\n",
    "    #the first number represents units -- in this case there are 128 units \n",
    "    model[name].add(Dense(len(chars[name]), activation = 'softmax')) \n",
    "    # add a Dense layer to the model, len(chars) units and a softmax activation function\n",
    "\n",
    "    optimizer = RMSprop(lr = 0.01) #this determines how gradient descent will be carried out to optimize your model\n",
    "\n",
    "    model[name].compile(loss='categorical_crossentropy',optimizer=optimizer) \n",
    "    # sets up the model so it can be trained,\n",
    "    # first defining a loss function, and then an optimizatio function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x13d583810>>\n",
      "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x13ee9bc10>>\n",
      "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x13fc00f90>>\n",
      "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x13fc42490>>\n"
     ]
    }
   ],
   "source": [
    "for name in nameList:\n",
    "    print(model[name].summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "def sent_transform(sent_string):\n",
    "    return nltk.word_tokenize(sent_string.lower())\n",
    "\n",
    "# words is a list of words\n",
    "def make_ngram_tuples(words, n):\n",
    "    for i in range(0,n):\n",
    "        words.insert(0, '<s>')\n",
    "    words.append('</s>')\n",
    "    result = []\n",
    "    x = range(n, len(words))\n",
    "    for i in x:\n",
    "        context = tuple(words[i-n+1:i])\n",
    "        sequence = (context,) + (words[i],)\n",
    "        result.append(sequence)\n",
    "    return result\n",
    "\n",
    "class BigramModel(object):\n",
    "    \n",
    "    def __init__(self, inputfile):\n",
    "        self.d = Counter() # keep track of # of words\n",
    "        \n",
    "        # iterating through the text file and incrementing the count for the words seen\n",
    "        text = inputfile\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        for line in sentences:\n",
    "            if(line != '\\n'):\n",
    "                list_words = sent_transform(line)\n",
    "                ngram_sequence = make_ngram_tuples(list_words, 2)\n",
    "                for e in ngram_sequence:\n",
    "                    self.d[\"\".join(e[0])] += 1\n",
    "                    self.d[e[1]] += 1\n",
    "        \n",
    "        self.unk_set = set() # set containing words that appear only once\n",
    "        self.dsum = 0 # the size of the vocabulary\n",
    "        \n",
    "        for word in self.d:\n",
    "            if self.d[word] == 1:\n",
    "                self.unk_set.add(word)\n",
    "            else:\n",
    "                self.dsum += 1\n",
    "        \n",
    "        self.d_bigram = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        text = inputfile\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        for line in sentences:\n",
    "            list_words = sent_transform(line) \n",
    "            ngram_sequence = make_ngram_tuples(list_words, 2)\n",
    "            for e in ngram_sequence:\n",
    "                context = \"\".join(e[0]) if \"\".join(e[0]) not in self.unk_set else \"<UNK>\"\n",
    "                word = e[1] if e[1] not in self.unk_set else \"<UNK>\"\n",
    "                self.d_bigram[context][word] += 1\n",
    "    \n",
    "    def logprob(self, context, word): \n",
    "        if context in self.unk_set:\n",
    "            context = \"<UNK>\"\n",
    "        if word in self.unk_set:\n",
    "            word = \"<UNK>\"\n",
    "        return np.log2(self.d_bigram[context][word]+1)-np.log2(self.d[context]+self.dsum)\n",
    "    \n",
    "    def get_ppl(self, testfile):\n",
    "        log_corpus_prob = 0\n",
    "        len_corpus = 0\n",
    "        f = testfile\n",
    "        for line in f:\n",
    "            if(line != '\\n'):\n",
    "                list_words = sent_transform(line)\n",
    "                ngram_sequence = make_ngram_tuples(list_words, 2)\n",
    "                for e in ngram_sequence:\n",
    "                    context = \"\".join(e[0]) if \"\".join(e[0]) not in self.unk_set else \"<UNK>\"\n",
    "                    word = e[1] if e[1] not in self.unk_set else \"<UNK>\"\n",
    "                    log_corpus_prob += self.logprob(context, word)\n",
    "                len_corpus += len(list_words)\n",
    "        return 2 ** (-1 * log_corpus_prob/len_corpus)\n",
    "    \n",
    "#perplexity_model = BigramModel(devtext) testing on the development set for tuning hyperparameters\n",
    "\n",
    "perplexity_model = {}\n",
    "for name in nameList:\n",
    "    perplexity_model[name] = BigramModel(testtext[name]) # now the data is being tested on the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0): #whichever character has the highest likelyhood is the character that gets printed\n",
    "    \n",
    "    # temperature - determines how crazy the program can get\n",
    "    \n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    \n",
    "    \n",
    "    start_index = random.randint(0, len(processed_text[name]) - maxlen - 1)\n",
    "    #for temperature in [0.2, 0.5, 1.0, 1.2]: testing temperature for the development sets\n",
    "    temperature = 0.2\n",
    "    print(\"------------------temperature:\", temperature)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = processed_text[name][start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('------------------generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # take a random (maxlen) fragment of the lyrics -- this will be your start\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars[name])))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[name][char]] = 1\n",
    "        # turns the seed into a vector\n",
    "\n",
    "        preds = model[name].predict(x_pred, verbose=0)[0] # predict the next character \n",
    "        next_index = sample(preds, temperature) #get the index of the character\n",
    "        next_char = indices_char[name][next_index] #get the character\n",
    "\n",
    "        generated += next_char #add the next character\n",
    "        sentence = sentence[1:] + next_char # shift the \"sentence\" that will be used to predict\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print()\n",
    "    print(\"perplexity: \" , perplexity_model[name].get_ppl(generated)) #get perplexity\n",
    "    print(\"\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end = on_epoch_end)\n",
    "\n",
    "\n",
    "for name in nameList:\n",
    "    print(\"Generating Lyrics for:\" + name)\n",
    "    \n",
    "    model[name].fit(x[name], y[name],\n",
    "             batch_size=128,\n",
    "             epochs=4,\n",
    "             callbacks=[print_callback])\n",
    "\n",
    "# an epoch is a round of training\n",
    "# a batch_size is how many samples are used in each epoch, \n",
    "# because you don't want to use every single lyric sample during\n",
    "# training\n",
    "\n",
    "# and finally, the callback just makes it so the function is\n",
    "# ran after each epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source for neural network code: \n",
    "\n",
    "Jeff Heaton. 2019. Text Generation with Keras and TensorFlow (10.3). Youtube. https://www.youtube.com/watch?v=6ORnRAz3gnA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
